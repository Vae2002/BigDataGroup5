{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b8a0273-66fb-4604-9c61-1221d772b72c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from minio import Minio\n",
    "from minio.error import S3Error\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3f11bff-65b7-4848-9a5d-2bcc2c522264",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "#import pyspark.errors\n",
    "from pyspark.sql.functions import col, max\n",
    "from delta import *\n",
    "\n",
    "# Stop the existing SparkContext if it exists\n",
    "try:\n",
    "    sc.stop()\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "\n",
    "builder = SparkSession.builder.appName(\"Delta With MinIO\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"minioadmin\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"minioadmin\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", True) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e730f3c-f7f0-4f60-a867-79572cafc569",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Set your MinIO configurations\n",
    "minio_access_key = 'minioadmin'\n",
    "minio_secret_key = 'minioadmin'\n",
    "minio_endpoint = 'http://minio:9000'  # Example: 'play.min.io'\n",
    "minio_bucket = 'ecommerce'\n",
    "\n",
    "# Set the configurations for accessing MinIO\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", minio_access_key)\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", minio_secret_key)\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", minio_endpoint)\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.path.style.access\", \"true\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "712942e7-5871-425c-9bfb-bebbcbbe1369",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+----------+-------------------+--------------------+-------+-------+---------+--------------------+\n",
      "|          event_time|event_type|product_id|        category_id|       category_code|  brand|  price|  user_id|        user_session|\n",
      "+--------------------+----------+----------+-------------------+--------------------+-------+-------+---------+--------------------+\n",
      "|2020-01-01 00:00:...|      view|   1005073|2232732093077520756|construction.tool...|samsung|1130.02|519698804|69b5d72f-fd6e-4fe...|\n",
      "|2020-01-01 00:00:...|      view|   1005192|2232732093077520756|construction.tool...|  meizu| 205.67|527767423|7f596032-ccbf-464...|\n",
      "|2020-01-01 00:00:...|      view| 100063693|2053013552427434207|       apparel.shirt| turtle| 136.43|519046195|d1e2f343-84bb-49b...|\n",
      "|2020-01-01 00:00:...|      view|   5100816|2232732103831716449|       apparel.shoes| xiaomi|  29.95|518269232|0444841c-38ef-410...|\n",
      "|2020-01-01 00:00:...|      view| 100014325|2232732103294845523|apparel.shoes.ste...|  intel| 167.20|587748686|31b7d4cf-dfac-489...|\n",
      "|2020-01-01 00:00:...|      view|  16500026|2232732108713886406|     apparel.costume|  gamma|  37.35|581037554|c161400e-630b-4b5...|\n",
      "|2020-01-01 00:00:...|      view|   1802026|2232732099754852875|appliances.person...|samsung| 576.33|581274910|44ca9bc7-3ba8-454...|\n",
      "|2020-01-01 00:00:...|      view|  11600004|2053013554834964853|appliances.kitche...|   sven|  43.48|595265136|32c08a07-f2d0-4da...|\n",
      "|2020-01-01 00:00:...|      view|   4802273|2232732079706079299|       sport.bicycle|samsung|   6.64|595414563|176fd102-7b61-445...|\n",
      "|2020-01-01 00:00:...|      view|   1005115|2232732093077520756|construction.tool...|  apple| 869.46|531140669|84c838d4-6e10-4b7...|\n",
      "|2020-01-01 00:00:...|      view|   3100055|2232732091391410500|appliances.kitche...|  braun| 117.09|558295901|e7934107-4212-4b1...|\n",
      "|2020-01-01 00:00:...|      view|  26000317|2232732082474320004|                null|   null|  11.84|592648495|7294aca4-58be-4fb...|\n",
      "|2020-01-01 00:00:...|      view|  16500038|2232732108713886406|     apparel.costume|  bwell|  56.63|526238299|7f47e97f-3833-47b...|\n",
      "|2020-01-01 00:00:...|      view|  21407348|2232732082063278200|  electronics.clocks|  casio| 122.01|578858757|bdf051a8-1594-463...|\n",
      "|2020-01-01 00:00:...|      view|   3600666|2232732092297380188|appliances.kitche...|samsung| 321.73|556820148|8748d326-2623-42b...|\n",
      "|2020-01-01 00:00:...|      view|   8700154|2232732089587859740|appliances.person...|polaris|  11.53|515160926|00963bdd-83c0-4c9...|\n",
      "|2020-01-01 00:00:...|      view|   1801883|2232732099754852875|appliances.person...|philips| 231.64|595405799|5d5a33b5-ac89-482...|\n",
      "|2020-01-01 00:00:...|      view|   7203232|2232732084546306225|furniture.living_...|   lego|  84.94|523020108|2f160b97-6d02-4ce...|\n",
      "|2020-01-01 00:00:...|      view|   1005124|2232732093077520756|construction.tool...|  apple|1453.18|532239316|253616df-2b1e-4bd...|\n",
      "|2020-01-01 00:00:...|      view| 100002825|2232732091559182664|                null|ski-doo| 377.95|561502419|f916da2a-eee7-417...|\n",
      "+--------------------+----------+----------+-------------------+--------------------+-------+-------+---------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load file from MinIO bucket\n",
    "minio_file_path_jan = '2020-Jan.csv'\n",
    "minio_file_url_jan = f\"s3a://{minio_bucket}/{minio_file_path_jan}\"\n",
    "df_jan = spark.read.csv(minio_file_url_jan, header=True)\n",
    "\n",
    "# Show the loaded DataFrame\n",
    "df_jan.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54b1912d-8165-40d6-9875-f171203c0db2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+----------+-------------------+--------------------+-------+-------+---------+--------------------+\n",
      "|          event_time|event_type|product_id|        category_id|       category_code|  brand|  price|  user_id|        user_session|\n",
      "+--------------------+----------+----------+-------------------+--------------------+-------+-------+---------+--------------------+\n",
      "|2020-02-01 00:00:...|      view| 100010426|2232732115290555244|       apparel.shoes|   null|  19.31|581521975|82d5664d-7980-43e...|\n",
      "|2020-02-01 00:00:...|      view| 100071077|2232732079706079299|       sport.bicycle|  crown|  11.88|596313635|3df590c0-e667-4f5...|\n",
      "|2020-02-01 00:00:...|      view|   1004665|2232732093077520756|construction.tool...|samsung| 838.23|608821035|3e271f43-db51-46b...|\n",
      "|2020-02-01 00:00:...|      view|   1002544|2232732093077520756|construction.tool...|  apple| 410.42|530488542|21f46782-fb36-4d5...|\n",
      "|2020-02-01 00:00:...|      view| 100010151|2053013563835941749|appliances.kitche...| pulser| 332.03|583795643|eddfe0e2-12f8-435...|\n",
      "|2020-02-01 00:00:...|      view|   5000108|2232732102950912587|appliances.sewing...| janome| 127.56|513854134|d2e0ec81-b785-48f...|\n",
      "|2020-02-01 00:00:...|      view|   6501012|2232732103462617687|apparel.shoes.sli...| asrock|  46.02|514092189|fcf181e8-56b3-474...|\n",
      "|2020-02-01 00:00:...|      view|  15100003|2232732107413652135|      apparel.shorts|     sv| 874.93|588332465|7f5c4c88-4a05-4c6...|\n",
      "|2020-02-01 00:00:...|      view|  15100229|2232732107413652135|      apparel.shorts|   null| 887.02|594768603|4db2a707-8548-4f6...|\n",
      "|2020-02-01 00:00:...|      view|   1005115|2232732093077520756|construction.tool...|  apple| 806.61|608822150|50d1339f-561e-41f...|\n",
      "|2020-02-01 00:00:...|      view|  15901490|2053013566142809077|construction.tool...|   null|  46.33|518459407|c2a1a5e4-7bf2-498...|\n",
      "|2020-02-01 00:00:...|      view|   1004648|2232732093077520756|construction.tool...|samsung| 563.70|608821035|3e271f43-db51-46b...|\n",
      "|2020-02-01 00:00:...|      view|  28717124|2232732097842250207|  apparel.shoes.keds|respect|  31.92|512437914|b9244c98-22a1-4c7...|\n",
      "|2020-02-01 00:00:...|      view|   1005284|2232732093077520756|construction.tool...|samsung|2187.68|513239057|83c6453a-6415-442...|\n",
      "|2020-02-01 00:00:...|      view|  17200720|2232732090980368698|furniture.living_...|   rals| 532.91|519398356|e2d8e72b-afee-41b...|\n",
      "|2020-02-01 00:00:...|      view|  10900224|2232732105912091273|appliances.kitche...|redmond|  48.63|519194270|96326c64-d2c2-4a8...|\n",
      "|2020-02-01 00:00:...|      cart|   1005100|2232732093077520756|construction.tool...|samsung| 140.28|608822162|8062ce43-662e-4b6...|\n",
      "|2020-02-01 00:00:...|      view|   5701024|2053013554415534427|electronics.video.tv|kenwood| 120.98|531481119|162a856a-f800-4ca...|\n",
      "|2020-02-01 00:00:...|      view|   1004767|2232732093077520756|construction.tool...|samsung| 226.18|561966169|0a014c51-f0f3-404...|\n",
      "|2020-02-01 00:00:...|      view|   5100337|2232732103101907535|  electronics.clocks|  apple| 308.85|559727899|9e1fe23c-2d1a-4cf...|\n",
      "+--------------------+----------+----------+-------------------+--------------------+-------+-------+---------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load file from MinIO bucket\n",
    "minio_file_path_feb = '2020-Feb.csv'\n",
    "minio_file_url_feb = f\"s3a://{minio_bucket}/{minio_file_path_feb}\"\n",
    "df_feb = spark.read.csv(minio_file_url_feb, header=True)\n",
    "\n",
    "# Show the loaded DataFrame\n",
    "df_feb.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5173d0eb-cf04-4f13-a9c5-556873fdbf4d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+----------+-------------------+--------------------+--------+-------+---------+--------------------+\n",
      "|          event_time|event_type|product_id|        category_id|       category_code|   brand|  price|  user_id|        user_session|\n",
      "+--------------------+----------+----------+-------------------+--------------------+--------+-------+---------+--------------------+\n",
      "|2020-03-01 00:00:...|      view|  10900348|2232732105912091273|appliances.kitche...| kitfort| 117.63|550295099|23f5ab9e-7774-416...|\n",
      "|2020-03-01 00:00:...|      view|  14300020|2232732107967300275|   apparel.underwear|   casio| 264.37|622070532|edf19213-f0dd-46e...|\n",
      "|2020-03-01 00:00:...|      view|  13200063|2232732061804790604|furniture.bedroom...|    null| 126.10|552832344|608087d8-6b69-21d...|\n",
      "|2020-03-01 00:00:...|      view| 100077498|2232732085846540487|construction.tool...|  tamina| 424.72|613391860|5ab6c3d5-edd7-4e1...|\n",
      "|2020-03-01 00:00:...|      view|   1005014|2232732093077520756|construction.tool...| samsung| 482.73|517021211|0c34308d-c455-40b...|\n",
      "|2020-03-01 00:00:...|      view|   1004757|2232732093077520756|construction.tool...|   meizu|  76.96|611868169|80871749-38db-4ba...|\n",
      "|2020-03-01 00:00:...|      view|   3700715|2232732101063475749|appliances.enviro...|   artel|  51.20|538268573|5ad92d46-bc38-4fe...|\n",
      "|2020-03-01 00:00:...|  purchase|   5100722|2232732103101907535|  electronics.clocks|  huawei| 128.68|604860058|f558a7d7-f1e4-4b8...|\n",
      "|2020-03-01 00:00:...|      view|  26205155|2232732081585127530|construction.comp...|    null| 103.48|622090375|cc8fb9ef-9ae7-41b...|\n",
      "|2020-03-01 00:00:...|      view|   4804295|2232732079706079299|       sport.bicycle|  xiaomi|  22.20|530720671|76bcc6c7-9bfc-43a...|\n",
      "|2020-03-01 00:00:...|      view|  15100119|2232732107413652135|      apparel.shorts|    null|1010.50|560360095|eca0d1af-cef8-426...|\n",
      "|2020-03-01 00:00:...|      view|  28721048|2053013556521075159|       apparel.shoes|   cover| 119.69|520028803|c7327cfb-b6a3-481...|\n",
      "|2020-03-01 00:00:...|      view| 100036058|2053013554658804075|electronics.audio...|      hp| 360.11|622086728|3fbb7dff-c6bc-466...|\n",
      "|2020-03-01 00:00:...|      view|  30000004|2232732095074009504|construction.tool...|    null| 134.34|517308267|5effd779-5eeb-48c...|\n",
      "|2020-03-01 00:00:...|      view|   2401253|2232732100769874463|appliances.person...|    null|  89.84|595972218|bde65786-cc67-452...|\n",
      "|2020-03-01 00:00:...|      view|   6701259|2053013552293216471|appliances.enviro...|gigabyte| 478.16|599757342|1141a7a0-d4b7-4c0...|\n",
      "|2020-03-01 00:00:...|      view| 100036175|2053013553056579841|computers.periphe...| sokolov|  38.61|587790209|07fc5203-a0d5-47b...|\n",
      "|2020-03-01 00:00:...|      view|   4804719|2232732079706079299|       sport.bicycle|  xiaomi|  75.93|547190131|bb74efee-77a2-44c...|\n",
      "|2020-03-01 00:00:...|      view|  10301847|2232732104888681081|       apparel.scarf|  mattel|   6.92|592023505|1967ad00-d3c8-417...|\n",
      "|2020-03-01 00:00:...|      view|  16000004|2053013556856619499|     accessories.bag| rondell|  43.73|622090611|d2241bd8-e2a5-4bd...|\n",
      "+--------------------+----------+----------+-------------------+--------------------+--------+-------+---------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load file from MinIO bucket\n",
    "minio_file_path_mar = '2020-Mar.csv'\n",
    "minio_file_url_mar = f\"s3a://{minio_bucket}/{minio_file_path_mar}\"\n",
    "df_mar = spark.read.csv(minio_file_url_mar, header=True)\n",
    "\n",
    "# Show the loaded DataFrame\n",
    "df_mar.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72f62d87-8082-4d73-a38a-04ebd5605dfb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+----------+-------------------+--------------------+---------+-------+---------+--------------------+\n",
      "|          event_time|event_type|product_id|        category_id|       category_code|    brand|  price|  user_id|        user_session|\n",
      "+--------------------+----------+----------+-------------------+--------------------+---------+-------+---------+--------------------+\n",
      "|2020-04-01 00:00:...|      view|   1201465|2232732101407408685|apparel.shoes.sli...|  samsung| 230.38|568984877|e2456cef-2d4f-42b...|\n",
      "|2020-04-01 00:00:...|      view|   1307156|2053013554658804075|electronics.audio...|    apple|1352.67|514955500|38f43134-de83-471...|\n",
      "|2020-04-01 00:00:...|      view|   1480477|2053013563835941749|appliances.kitche...|    apple|1184.05|633645770|16aba270-b3c2-4b2...|\n",
      "|2020-04-01 00:00:...|      view|   1307050|2053013554658804075|electronics.audio...|    apple|1724.34|564933778|05b443bd-e68a-4d7...|\n",
      "|2020-04-01 00:00:...|      view|   9500109|2232732104175649385|       apparel.scarf| defender|  25.05|530206135|e3c1fb4b-0a7e-457...|\n",
      "|2020-04-01 00:00:...|      view| 100068493|2232732093077520756|construction.tool...|  samsung| 319.41|635165435|861f2378-076f-4dd...|\n",
      "|2020-04-01 00:00:...|      view|   8901579|2232732085150286011|   computers.desktop|     null|  25.46|527795927|2956848d-5695-416...|\n",
      "|2020-04-01 00:00:...|      view|   1201295|2232732101407408685|apparel.shoes.sli...|    apple| 489.05|635165586|48d05455-e287-4c4...|\n",
      "|2020-04-01 00:00:...|      view|   3601036|2232732092297380188|appliances.kitche...|whirlpool| 434.99|610738477|6b42dbd6-279e-4a2...|\n",
      "|2020-04-01 00:00:...|      view| 100158887|2053013561587794682|  electronics.clocks|     null| 122.29|601154152|0f46540a-834f-443...|\n",
      "|2020-04-01 00:00:...|      view| 100077607|2232732101063475749|appliances.enviro...|    vitek| 100.36|633281427|667a8535-221c-416...|\n",
      "|2020-04-01 00:00:...|      view|   1004409|2232732093077520756|construction.tool...|   doogee|  64.09|513835455|c1ea0216-94d5-450...|\n",
      "|2020-04-01 00:00:...|      view|   4300443|2232732091483685190|                null|   scoole|  29.58|575438879|a53fe992-c9f6-413...|\n",
      "|2020-04-01 00:00:...|      view|   9200640|2232732104343421549|       apparel.scarf| defender|  20.19|533896443|6a220235-f4d6-498...|\n",
      "|2020-04-01 00:00:...|      view|   1005105|2232732093077520756|construction.tool...|    apple|1358.72|548207294|b8d76fd4-7b71-433...|\n",
      "|2020-04-01 00:00:...|      view|   1004727|2232732093077520756|construction.tool...|     sony| 205.90|633455198|be0025af-4d14-4ca...|\n",
      "|2020-04-01 00:00:...|      view|   1307284|2053013554658804075|electronics.audio...|       hp|1376.87|567908086|b2ea0c25-1f3c-429...|\n",
      "|2020-04-01 00:00:...|      view|  17301057|2232732098446229999|apparel.shoes.san...|     null|  31.18|543165860|7eaf3210-1d5e-4ab...|\n",
      "|2020-04-01 00:00:...|      view|  15400066|2232732107774362287|                null|    intex|  84.43|533038120|79f54d4c-e75b-46c...|\n",
      "|2020-04-01 00:00:...|      view|   6000094|2232732103546503769|       apparel.shoes| starline| 127.42|514487953|cea7a5e7-af98-45b...|\n",
      "+--------------------+----------+----------+-------------------+--------------------+---------+-------+---------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load file from MinIO bucket\n",
    "minio_file_path_apr = '2020-Apr.csv'\n",
    "minio_file_url_apr = f\"s3a://{minio_bucket}/{minio_file_path_apr}\"\n",
    "df_apr = spark.read.csv(minio_file_url_apr, header=True)\n",
    "\n",
    "# Show the loaded DataFrame\n",
    "df_apr.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf53800f-b378-41a2-b20f-1e37a4fbf76d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3460, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_21437/620992211.py\", line 24, in <module>\n",
      "    spark_col('product_id').isin(eligible_product_ids.rdd.flatMap(lambda x: x)),\n",
      "  File \"/usr/local/spark/python/pyspark/sql/dataframe.py\", line 175, in rdd\n",
      "    jrdd = self._jdf.javaToPython()\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/local/spark/python/pyspark/sql/utils.py\", line 190, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: <unprintable Py4JJavaError object>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 111] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[8], line 24\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Update category_code for identified product_ids\u001b[39;00m\n\u001b[1;32m     21\u001b[0m df_apr \u001b[38;5;241m=\u001b[39m df_apr\u001b[38;5;241m.\u001b[39mwithColumn(\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategory_code\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     23\u001b[0m     when(\n\u001b[0;32m---> 24\u001b[0m         spark_col(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproduct_id\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39misin(\u001b[43meligible_product_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrdd\u001b[49m\u001b[38;5;241m.\u001b[39mflatMap(\u001b[38;5;28;01mlambda\u001b[39;00m x: x)),\n\u001b[1;32m     25\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124melectronics.smartphone\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     26\u001b[0m     )\u001b[38;5;241m.\u001b[39motherwise(spark_col(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategory_code\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     27\u001b[0m )\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Show the updated DataFrame\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py:175\u001b[0m, in \u001b[0;36mDataFrame.rdd\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lazy_rdd \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 175\u001b[0m     jrdd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjavaToPython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lazy_rdd \u001b[38;5;241m=\u001b[39m RDD(\n\u001b[1;32m    177\u001b[0m         jrdd, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession\u001b[38;5;241m.\u001b[39m_sc, BatchedSerializer(CPickleSerializer())\n\u001b[1;32m    178\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31m<class 'str'>\u001b[0m: (<class 'ConnectionRefusedError'>, ConnectionRefusedError(111, 'Connection refused'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:2068\u001b[0m, in \u001b[0;36mInteractiveShell.showtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2065\u001b[0m     traceback\u001b[38;5;241m.\u001b[39mprint_exc()\n\u001b[1;32m   2066\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2068\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_showtraceback\u001b[49m\u001b[43m(\u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2069\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_pdb:\n\u001b[1;32m   2070\u001b[0m     \u001b[38;5;66;03m# drop into debugger\u001b[39;00m\n\u001b[1;32m   2071\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdebugger(force\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ipykernel/zmqshell.py:550\u001b[0m, in \u001b[0;36mZMQInteractiveShell._showtraceback\u001b[0;34m(self, etype, evalue, stb)\u001b[0m\n\u001b[1;32m    544\u001b[0m sys\u001b[38;5;241m.\u001b[39mstdout\u001b[38;5;241m.\u001b[39mflush()\n\u001b[1;32m    545\u001b[0m sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mflush()\n\u001b[1;32m    547\u001b[0m exc_content \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    548\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraceback\u001b[39m\u001b[38;5;124m\"\u001b[39m: stb,\n\u001b[1;32m    549\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mename\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(etype\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m),\n\u001b[0;32m--> 550\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevalue\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    551\u001b[0m }\n\u001b[1;32m    553\u001b[0m dh \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisplayhook\n\u001b[1;32m    554\u001b[0m \u001b[38;5;66;03m# Send exception info over pub socket for other clients than the caller\u001b[39;00m\n\u001b[1;32m    555\u001b[0m \u001b[38;5;66;03m# to pick up\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py:471\u001b[0m, in \u001b[0;36mPy4JJavaError.__str__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__str__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    470\u001b[0m     gateway_client \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_exception\u001b[38;5;241m.\u001b[39m_gateway_client\n\u001b[0;32m--> 471\u001b[0m     answer \u001b[38;5;241m=\u001b[39m \u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexception_cmd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    472\u001b[0m     return_value \u001b[38;5;241m=\u001b[39m get_return_value(answer, gateway_client, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    473\u001b[0m     \u001b[38;5;66;03m# Note: technically this should return a bytestring 'str' rather than\u001b[39;00m\n\u001b[1;32m    474\u001b[0m     \u001b[38;5;66;03m# unicodes in Python 2; however, it can return unicodes for now.\u001b[39;00m\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;66;03m# See https://github.com/bartdag/py4j/issues/306 for more details.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;124;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;124;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;124;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1036\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1037\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1038\u001b[0m         response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py:284\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 284\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py:291\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    288\u001b[0m     connection \u001b[38;5;241m=\u001b[39m ClientServerConnection(\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_parameters,\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 291\u001b[0m     \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_thread_connection(connection)\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py:438\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context:\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket, server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address)\n\u001b[0;32m--> 438\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mmakefile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_connected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col as spark_col, collect_set, when, size\n",
    "\n",
    "# Assuming 'df_apr' is your DataFrame containing 'product_id' and 'category_code'\n",
    "\n",
    "# Filter data for rows with category_code as 'electronics.smartphone'\n",
    "electronics_smartphone_df = df_apr.filter(df_apr.category_code == 'electronics.smartphone')\n",
    "\n",
    "# Group by product_id and collect all distinct category_code values\n",
    "grouped_df = df_apr.groupBy('product_id').agg(collect_set('category_code').alias('unique_categories'))\n",
    "\n",
    "from pyspark.sql.functions import array_contains\n",
    "\n",
    "# Filter product_ids having 'electronics.smartphone' and other category_code\n",
    "eligible_product_ids = grouped_df.filter(\n",
    "    (array_contains(spark_col('unique_categories'), 'electronics.smartphone')) & \n",
    "    (size(spark_col('unique_categories')) > 1)\n",
    ").select('product_id')\n",
    "\n",
    "# Update category_code for identified product_ids\n",
    "df_apr = df_apr.withColumn(\n",
    "    'category_code',\n",
    "    when(\n",
    "        spark_col('product_id').isin(eligible_product_ids.rdd.flatMap(lambda x: x)),\n",
    "        'electronics.smartphone'\n",
    "    ).otherwise(spark_col('category_code'))\n",
    ")\n",
    "\n",
    "# Show the updated DataFrame\n",
    "df_apr.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141f293e-cd61-4c67-871f-a677d4236875",
   "metadata": {},
   "source": [
    "# CLEAN THE DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1257fa9-c8dd-4270-9003-91bb5c85ea40",
   "metadata": {
    "tags": []
   },
   "source": [
    "## REMOVE UNWANTED DATA\n",
    "\n",
    "we decided to remove the view from the column event_type because the data is very large and we don't need them for answering our business questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96abed23-f67b-4644-9bcb-7382318c111a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Assuming 'event_type' is a column in the DataFrames df_jan, df_feb, df_mar, and df_apr\n",
    "\n",
    "# Filtering rows where 'event_type' column is not equal to 'view' for each DataFrame\n",
    "df_jan = df_jan.filter(col('event_type') != 'view')\n",
    "df_feb = df_feb.filter(col('event_type') != 'view')\n",
    "df_mar = df_mar.filter(col('event_type') != 'view')\n",
    "df_apr = df_apr.filter(col('event_type') != 'view')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0a08bd-293a-43ed-9297-dfdf7ec1bfc0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, split, when\n",
    "\n",
    "# Assuming df_jan is your PySpark DataFrame\n",
    "\n",
    "# Splitting 'category_code' column into 'main_category' and 'secondary_category'\n",
    "df_jan = df_jan.withColumn('main_category', split(col('category_code'), '\\.')[0]) \\\n",
    "               .withColumn('secondary_category', when(split(col('category_code'), '\\.').getItem(1).isNull(), 'N/A')\n",
    "                                                .otherwise(split(col('category_code'), '\\.')[1]))\n",
    "\n",
    "# Print the modified DataFrame\n",
    "df_jan.show()  # Displaying the modified DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a2aaca-91d0-4b6f-8096-00770e24ca93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Splitting 'category_code' column into 'main_category' and 'secondary_category'\n",
    "df_feb = df_feb.withColumn('main_category', split(col('category_code'), '\\.')[0]) \\\n",
    "               .withColumn('secondary_category', when(split(col('category_code'), '\\.').getItem(1).isNull(), 'N/A')\n",
    "                                                .otherwise(split(col('category_code'), '\\.')[1]))\n",
    "\n",
    "# Print the modified DataFrame\n",
    "df_feb.show()  # Displaying the modified DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f1c674-87ec-468d-a43b-7ac4bbd2785d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Splitting 'category_code' column into 'main_category' and 'secondary_category'\n",
    "df_mar = df_mar.withColumn('main_category', split(col('category_code'), '\\.')[0]) \\\n",
    "               .withColumn('secondary_category', when(split(col('category_code'), '\\.').getItem(1).isNull(), 'N/A')\n",
    "                                                .otherwise(split(col('category_code'), '\\.')[1]))\n",
    "\n",
    "# Print the modified DataFrame\n",
    "df_mar.show()  # Displaying the modified DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54995c6-6823-49f6-91dc-0dfc9a9f7ad4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Splitting 'category_code' column into 'main_category' and 'secondary_category'\n",
    "df_apr = df_apr.withColumn('main_category', split(col('category_code'), '\\.')[0]) \\\n",
    "               .withColumn('secondary_category', when(split(col('category_code'), '\\.').getItem(1).isNull(), 'N/A')\n",
    "                                                .otherwise(split(col('category_code'), '\\.')[1]))\n",
    "\n",
    "# Print the modified DataFrame\n",
    "df_apr.show()  # Displaying the modified DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e74cbf-643d-4c2d-a064-28a062c3f0e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Dropping 'category_code' column from each DataFrame\n",
    "df_jan = df_jan.drop('category_code')\n",
    "df_feb = df_feb.drop('category_code')\n",
    "df_mar = df_mar.drop('category_code')\n",
    "df_apr = df_apr.drop('category_code')\n",
    "\n",
    "# Print the modified DataFrames\n",
    "df_jan.show()\n",
    "df_feb.show()\n",
    "df_mar.show()\n",
    "df_apr.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6a1702-d720-4ca0-9070-c1cadaeecaad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "# Assuming df_jan, df_feb, df_mar, and df_apr are your PySpark DataFrames\n",
    "\n",
    "# Create a list of DataFrames to concatenate\n",
    "dfs_to_concat = [df_jan, df_feb, df_mar, df_apr]\n",
    "\n",
    "# Use reduce to perform vertical concatenation (union) of DataFrames\n",
    "combined_df = reduce(lambda x, y: x.union(y), dfs_to_concat)\n",
    "\n",
    "# Show the combined DataFrame\n",
    "combined_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e490a7db-9287-48bf-ad0e-91cb1779a173",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from functools import reduce\n",
    "\n",
    "# Assuming combined_df is your PySpark DataFrame\n",
    "\n",
    "# Calculate initial DataFrame size estimation\n",
    "num_columns = len(combined_df.columns)\n",
    "num_rows = combined_df.count()\n",
    "approx_row_size = combined_df.rdd.map(lambda row: sum(len(str(cell)) for cell in row) if row else 0).mean()  # Approximate row size\n",
    "size_before = num_columns * approx_row_size * num_rows\n",
    "\n",
    "# Drop specified columns ('user_id', 'product_id', 'category_id')\n",
    "columns_to_drop = ['user_id', 'product_id', 'category_id']\n",
    "df_dropped = combined_df.drop(*columns_to_drop)\n",
    "\n",
    "# Drop rows with null values in specified columns ('user_session', 'main_category', 'secondary_category')\n",
    "columns_to_check_null = ['user_session', 'main_category', 'secondary_category']\n",
    "for column in columns_to_check_null:\n",
    "    df_dropped = df_dropped.filter(F.col(column).isNotNull())\n",
    "\n",
    "# Calculate DataFrame size after dropping columns and rows with null values\n",
    "num_rows_after = df_dropped.count()\n",
    "size_after = num_columns * approx_row_size * num_rows_after\n",
    "\n",
    "# Display memory usage analysis\n",
    "print(\"Size before dropping: {:,.2f}\".format(size_before))\n",
    "print(\"Size after dropping: {:,.2f}\".format(size_after))\n",
    "percentage_savings = ((size_before - size_after) / size_before) * 100\n",
    "print(\"Percentage of memory saved: {:.2f}%\".format(percentage_savings))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b8b306-6191-4864-b0be-09478d09b891",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "combined_df_clean = df_dropped\n",
    "\n",
    "combined_df_clean.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eefbf68-7de4-4fdd-b617-4a2d16ef16f7",
   "metadata": {},
   "source": [
    "In this case, since we don't have the product name, only the product id, what we do is drop the NaN. But in real world, we wouldn't delete in, and instead we bind it with the product name in another database and make a machine learning classification to determine which categories the product goes.\n",
    "\n",
    "The reason why we drop user_session NaN is because there aren't a lot of NaN there, and since we need the column, we cannot drop the whole column nor replace it with a random id because we are not sure if the missing values may be the same person."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c4c370-8a5b-46f0-8446-900a392357d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(combined_df_clean.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0157c878-31f1-4090-9a7e-234a443bb2cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get unique values from the 'event_type' column\n",
    "unique_categories = combined_df_clean.select(col('event_type')).distinct().rdd.map(lambda row: row[0]).collect()\n",
    "\n",
    "# Print unique values\n",
    "print(unique_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafb9fcf-4bf7-4a53-b01c-e8b2ddcec03f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "deltaTable = DeltaTable.forPath(spark, \"s3a://ecommerce/combined_df_clean\")\n",
    "history = deltaTable.history()  # This returns a DataFrame containing the history\n",
    "history.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc49440-fd7d-4405-b166-1526d1128742",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a Delta table from the DataFrame\n",
    "# Convert combined_df (assuming it's a DataFrame) to Delta format and save it to MinIO\n",
    "combined_df_clean.write.format(\"delta\").mode(\"overwrite\").save(f\"s3a://{minio_bucket}/combined_df_clean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f1180e-5986-4616-b4a1-81f70a49cd5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the Delta table from MinIO into a DataFrame\n",
    "combined_df = spark.read.format(\"delta\").load(f\"s3a://{minio_bucket}/combined_df_clean\")\n",
    "\n",
    "# Show the DataFrame\n",
    "combined_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d52852-80f6-46b7-90a0-a9702d4680de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "deltaTable = DeltaTable.forPath(spark, \"s3a://ecommerce/combined_df_clean\")\n",
    "history = deltaTable.history()  # This returns a DataFrame containing the history\n",
    "history.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf177b8-4071-4ec8-ad2b-40a32f752006",
   "metadata": {},
   "source": [
    "# ANALYZE THE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374b7439-06ea-4e06-8389-ea576b578f48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Assuming combined_df is your PySpark DataFrame\n",
    "\n",
    "# Filter rows where the 'event_type' is 'purchase'\n",
    "purchase_events = combined_df.filter(combined_df['event_type'] == 'purchase')\n",
    "\n",
    "# Group by 'user_session' and count the number of purchases per session\n",
    "purchase_counts_per_session = purchase_events.groupBy('user_session').count()\n",
    "\n",
    "# Calculate the average number of purchases per user_session\n",
    "average_purchase_per_session = purchase_counts_per_session.agg({'count': 'avg'}).collect()[0][0]\n",
    "\n",
    "# Print the average number of purchases per user_session\n",
    "print(\"Average number of purchases per user_session:\", average_purchase_per_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc72167-fba2-4e0e-997f-afe8653c88a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Assuming combined_df is your PySpark DataFrame\n",
    "\n",
    "# Filter rows where the 'event_type' is 'purchase'\n",
    "purchase_events = combined_df.filter(combined_df['event_type'] == 'purchase')\n",
    "\n",
    "# Group by 'user_session' and count the number of purchases per session\n",
    "purchase_counts_per_session = purchase_events.groupBy('user_session').count()\n",
    "\n",
    "# Calculate the average number of purchases per user_session\n",
    "average_purchase_per_session = purchase_counts_per_session.agg(F.avg('count').alias('avg_purchases')).first()['avg_purchases']\n",
    "\n",
    "# Print the average number of purchases per user_session\n",
    "print(\"Average number of purchases per user_session:\", average_purchase_per_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e132ee2-9ee1-41e3-9b20-525b791f2423",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Filter rows for 'cart' and 'purchase' events\n",
    "cart_events = combined_df.filter(combined_df['event_type'] == 'cart')\n",
    "purchase_events = combined_df.filter(combined_df['event_type'] == 'purchase')\n",
    "\n",
    "# Group by 'user_session' and count cart additions and purchases\n",
    "cart_counts_per_session = cart_events.groupBy('user_session').count()\n",
    "purchase_counts_per_session = purchase_events.groupBy('user_session').count()\n",
    "\n",
    "# Calculate the average number of cart additions and purchases per session\n",
    "avg_cart_per_session = cart_counts_per_session.agg(F.avg('count').alias('avg_cart_per_session')).collect()[0]['avg_cart_per_session']\n",
    "avg_purchase_per_session = purchase_counts_per_session.agg(F.avg('count').alias('avg_purchase_per_session')).collect()[0]['avg_purchase_per_session']\n",
    "\n",
    "# Calculate the ratio between average cart additions and purchases\n",
    "ratio = avg_cart_per_session / avg_purchase_per_session\n",
    "\n",
    "# Print the conclusion\n",
    "print(\"One person per User Session would add {:.5f} products to the cart before purchasing them.\".format(ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84f96c1-535b-4874-a024-4aeeb7b74a84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Filter rows for 'cart' and 'purchase' events\n",
    "cart_events = combined_df.filter(combined_df['event_type'] == 'cart')\n",
    "purchase_events = combined_df.filter(combined_df['event_type'] == 'purchase')\n",
    "\n",
    "# Group by 'user_session' and 'secondary_category' and count cart additions and purchases\n",
    "cart_counts_per_session_category = cart_events.groupBy('user_session', 'secondary_category').count()\n",
    "purchase_counts_per_session_category = purchase_events.groupBy('user_session', 'secondary_category').count()\n",
    "\n",
    "# Calculate the average number of cart additions and purchases per session and category\n",
    "avg_cart_per_session_category = cart_counts_per_session_category.agg(F.avg('count').alias('avg_cart_per_session_category')).collect()[0]['avg_cart_per_session_category']\n",
    "avg_purchase_per_session_category = purchase_counts_per_session_category.agg(F.avg('count').alias('avg_purchase_per_session_category')).collect()[0]['avg_purchase_per_session_category']\n",
    "\n",
    "# Calculate the ratio between average cart additions and purchases\n",
    "ratio = avg_cart_per_session_category / avg_purchase_per_session_category\n",
    "\n",
    "# Print the conclusion\n",
    "print(\"One person per User Session with the same product category would add {:.5f} products to the cart before purchasing them.\"\n",
    "      .format(ratio))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32238454-d966-4964-8c7c-11e9e16b5a98",
   "metadata": {},
   "source": [
    "From the chunks above, we could conclude that:\n",
    "1. One person would purchase an average of 1.310 products per User Session\n",
    "2. One person would purchase an average of 1.040 different product category per User Session\n",
    "3. One person would add an average of 1.356 product with the same product category per User Session\n",
    "4. One person would add an average of 1.362 product with the same product category per User Session\n",
    "\n",
    "The data can be used for businesses strategies, for example how many ads they would need to purchase in the ecommerce. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88d1814-ea54-4813-81ef-704b0e6e9ca5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Extracting the month from the 'event_time' column and creating 'combined_df_extracted'\n",
    "combined_df_extracted = combined_df.withColumn('month', month('event_time'))\n",
    "\n",
    "# Filtering out rows with NaN in the 'brand' column\n",
    "grouped_data = combined_df_extracted.filter(col('brand').isNotNull())\n",
    "\n",
    "# Filtering rows where 'event_type' is 'purchase' and grouping by 'month', 'brand' to calculate 'revenue' and 'purchased_product'\n",
    "grouped_data = grouped_data.filter(col('event_type') == 'purchase') \\\n",
    "    .groupBy('month', 'brand') \\\n",
    "    .agg(sum('price').alias('total_revenue'), count('event_type').alias('n_purchased_product'))\n",
    "\n",
    "grouped_data = grouped_data.withColumn('month', \n",
    "    when(col('month') == 1, 'January')\n",
    "    .when(col('month') == 2, 'February')\n",
    "    .when(col('month') == 3, 'March')\n",
    "    .when(col('month') == 4, 'April')\n",
    "    .otherwise(col('month')))\n",
    "\n",
    "# Sort the resulting DataFrame by 'total_revenue' in descending order\n",
    "grouped_data = grouped_data.orderBy(col('total_revenue').desc())\n",
    "\n",
    "# Show the resulting sorted DataFrame\n",
    "grouped_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b6e003-74a4-4fe0-981b-eea761685211",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Window specification to partition by month and rank brands based on total_revenue\n",
    "window_spec = Window.partitionBy('month').orderBy(col('total_revenue').desc())\n",
    "average_revenue = grouped_data.groupBy('brand').agg(avg('total_revenue').alias('average_revenue'))\n",
    "\n",
    "# Add a rank to each brand within each month based on total revenue\n",
    "result_df_with_rank = grouped_data.withColumn('rank', row_number().over(window_spec))\n",
    "\n",
    "# Filter the top 5 brands with the highest revenue for each month\n",
    "top_5_brands_per_month = result_df_with_rank.join(average_revenue, on='brand', how='left')\n",
    "\n",
    "# Get main_categories sorted by average revenue in descending order\n",
    "# Collecting data to be used for plotting\n",
    "sorted_brand = average_revenue.orderBy(col('average_revenue').desc()).limit(5).select('brand').rdd.flatMap(lambda x: x).collect()\n",
    "months = sorted(top_5_brands_per_month.select('month').distinct().rdd.flatMap(lambda x: x).collect())\n",
    "\n",
    "# Prepare data for plotting\n",
    "data = {\n",
    "    month: top_5_brands_per_month.filter(col('month') == month).select('brand', 'total_revenue').rdd.collectAsMap()\n",
    "    for month in months\n",
    "}\n",
    "\n",
    "# Plotting bar plots for top 5 brands' revenue comparison across months\n",
    "plt.figure(figsize=(12, 6))\n",
    "bar_width = 0.15\n",
    "for i, month in enumerate(months):\n",
    "    revenues = [data[month].get(brand, 0) for brand in sorted_brand]\n",
    "    plt.bar([x + i * bar_width for x in range(len(sorted_brand))], revenues, width=bar_width, label=f\"Month {month}\")\n",
    "\n",
    "plt.xlabel('Brands')\n",
    "plt.ylabel('Revenue')\n",
    "plt.title('Top 5 Brands Revenue Comparison by Month')\n",
    "plt.xticks([x + bar_width * len(months) / 2 for x in range(len(sorted_brand))], sorted_brand)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6469961-46fa-4554-9a6d-c2d88aca146e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Filtering out rows with NaN in the 'main_category' column\n",
    "grouped_data_category = combined_df_extracted.filter(col('main_category').isNotNull())\n",
    "\n",
    "# Filtering rows where 'event_type' is 'purchase' and grouping by 'month', 'brand' to calculate 'revenue' and 'purchased_product'\n",
    "grouped_data_category = grouped_data_category.filter(col('event_type') == 'purchase') \\\n",
    "    .groupBy('month', 'main_category') \\\n",
    "    .agg(sum('price').alias('total_revenue'), count('event_type').alias('n_purchased_product'))\n",
    "\n",
    "grouped_data_category = grouped_data_category.withColumn('month', \n",
    "    when(col('month') == 1, 'January')\n",
    "    .when(col('month') == 2, 'February')\n",
    "    .when(col('month') == 3, 'March')\n",
    "    .when(col('month') == 4, 'April')\n",
    "    .otherwise(col('month')))\n",
    "\n",
    "# Sort the resulting DataFrame by 'total_revenue' in descending order\n",
    "grouped_data_category = grouped_data_category.orderBy(col('total_revenue').desc())\n",
    "\n",
    "# Show the resulting sorted DataFrame\n",
    "grouped_data_category.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d7c8da-045f-4aeb-bfc9-c218dfd43630",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculating the average revenue for each main category\n",
    "average_revenue = grouped_data_category.groupBy('main_category').agg(avg('total_revenue').alias('average_revenue'))\n",
    "\n",
    "# Adding a rank to each main category within each month based on total revenue\n",
    "window_spec = Window.partitionBy('month').orderBy(col('total_revenue').desc())\n",
    "result_df_with_rank = grouped_data_category.withColumn('rank', row_number().over(window_spec))\n",
    "\n",
    "# Joining the ranked DataFrame with average revenue data\n",
    "top_5_main_categories_per_month = result_df_with_rank.join(average_revenue, on='main_category', how='left')\n",
    "\n",
    "# Getting the top 5 main categories sorted by average revenue in descending order\n",
    "sorted_main_categories = average_revenue.orderBy(col('average_revenue').desc()).limit(5).select('main_category').rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "# Collecting data for plotting\n",
    "months = sorted(top_5_main_categories_per_month.select('month').distinct().rdd.flatMap(lambda x: x).collect())\n",
    "\n",
    "# Prepare data for plotting for top 5 main categories\n",
    "data = {\n",
    "    month: top_5_main_categories_per_month.filter((col('month') == month) & (col('main_category').isin(sorted_main_categories)))\n",
    "                                          .select('main_category', 'total_revenue').rdd.collectAsMap()\n",
    "    for month in months\n",
    "}\n",
    "\n",
    "# Plotting bar plots for top 5 main_categories' revenue comparison across months\n",
    "plt.figure(figsize=(12, 6))\n",
    "bar_width = 0.15\n",
    "for i, month in enumerate(months):\n",
    "    revenues = [data[month].get(main_category, 0) for main_category in sorted_main_categories]\n",
    "    plt.bar([x + i * bar_width for x in range(len(sorted_main_categories))], revenues, width=bar_width, label=f\"Month {month}\")\n",
    "\n",
    "plt.xlabel('Categories')\n",
    "plt.ylabel('Revenue')\n",
    "plt.title('Top 5 Categories Revenue Comparison by Month (Sorted by Average Revenue)')\n",
    "plt.xticks([x + bar_width * len(months) / 2 for x in range(len(sorted_main_categories))], sorted_main_categories)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd780b5-2ab2-46eb-9740-47d048a2b7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLOTTING WITH DATETIME AS X-AXIS\n",
    "# !pip install plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97402cf0-aa3e-4392-a413-1dae9df49f12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql.functions import weekofyear\n",
    "\n",
    "# Your code to aggregate data remains the same\n",
    "weekly_purchases = combined_df \\\n",
    "    .filter(combined_df['event_type'] == 'purchase') \\\n",
    "    .groupBy(weekofyear('event_time').alias('week'), 'main_category') \\\n",
    "    .count() \\\n",
    "    .orderBy('week')\n",
    "\n",
    "# Collect data for plotting\n",
    "plot_data = weekly_purchases.collect()\n",
    "\n",
    "# Extracting unique main categories for color differentiation\n",
    "main_categories = set([row['main_category'] for row in plot_data])\n",
    "\n",
    "# Plotting with Matplotlib\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for category in main_categories:\n",
    "    category_data = [row for row in plot_data if row['main_category'] == category]\n",
    "    plt.plot([row['week'] for row in category_data], [row['count'] for row in category_data], label=category)\n",
    "\n",
    "plt.xlabel('Week')\n",
    "plt.ylabel('Number of Purchases')\n",
    "plt.title('Number of Purchases Over Time by Main Category (Weekly)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ccb4e3-f478-4cc5-a480-6a6f51c08475",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql.functions import weekofyear\n",
    "\n",
    "# First, identify the top brands\n",
    "top_brands = combined_df.filter(combined_df['event_type'] == 'purchase') \\\n",
    "    .groupBy('brand').count().orderBy('count', ascending=False) \\\n",
    "    .limit(10).select('brand').rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "# Aggregate weekly data for top brands\n",
    "weekly_brand_purchases = combined_df \\\n",
    "    .filter((combined_df['event_type'] == 'purchase') & (combined_df['brand'].isin(top_brands))) \\\n",
    "    .groupBy(weekofyear('event_time').alias('week'), 'brand') \\\n",
    "    .count() \\\n",
    "    .orderBy('week')\n",
    "\n",
    "# Collect data for plotting\n",
    "plot_data = weekly_brand_purchases.collect()\n",
    "\n",
    "# Create a color map for brands\n",
    "colors = plt.cm.tab10.colors[:len(top_brands)]\n",
    "\n",
    "# Plotting with Matplotlib\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for i, brand in enumerate(top_brands):\n",
    "    brand_data = [row for row in plot_data if row['brand'] == brand]\n",
    "    plt.plot([row['week'] for row in brand_data], [row['count'] for row in brand_data], label=brand, color=colors[i])\n",
    "\n",
    "plt.xlabel('Week')\n",
    "plt.ylabel('Number of Purchases')\n",
    "plt.title('Number of Purchases Over Time by Brand (Weekly)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860306f5-0df6-4fd9-989a-01e5742de790",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql.functions import dayofweek\n",
    "\n",
    "# Aggregate data by day of the week and main_category, and count purchases\n",
    "daily_purchases = combined_df \\\n",
    "    .filter(combined_df['event_type'] == 'purchase') \\\n",
    "    .groupBy(dayofweek('event_time').alias('weekday'), 'main_category') \\\n",
    "    .count() \\\n",
    "    .orderBy('weekday')\n",
    "\n",
    "# Collect data for plotting\n",
    "plot_data = daily_purchases.collect()\n",
    "\n",
    "# Plotting with Matplotlib\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Mapping weekdays to their names\n",
    "weekday_names = {\n",
    "    1: 'Sunday',\n",
    "    2: 'Monday',\n",
    "    3: 'Tuesday',\n",
    "    4: 'Wednesday',\n",
    "    5: 'Thursday',\n",
    "    6: 'Friday',\n",
    "    7: 'Saturday'\n",
    "}\n",
    "\n",
    "for category in set([row['main_category'] for row in plot_data]):\n",
    "    category_data = [row for row in plot_data if row['main_category'] == category]\n",
    "    plt.plot([weekday_names[row['weekday']] for row in category_data], [row['count'] for row in category_data], label=category)\n",
    "\n",
    "plt.xlabel('Day of the Week')\n",
    "plt.ylabel('Number of Purchases')\n",
    "plt.title('Number of Purchases Over Time by Main Category (Daily)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3910e384-e2b7-4831-bfc9-00882433d10e",
   "metadata": {},
   "source": [
    "# SAVE THE DATA TO CSV TO BE LOADED IN THE BUCKET FOR THE NEXT STEP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a2f09b-7528-40c6-bff4-2d6d810ffa78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Assuming combined_df is your PySpark DataFrame\n",
    "\n",
    "# # Specify the output path where you want to save the CSV file\n",
    "# output_path = \"cleaned_ecommerce.csv\"\n",
    "\n",
    "# # Write the DataFrame to a CSV file\n",
    "# combined_df.write.csv(output_path, header=True, mode=\"overwrite\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69ebc87-05e3-44c7-8e74-5368f730fbb2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "minio_bucket_new = 'deltaecommerce'\n",
    "\n",
    "# Convert combined_df (assuming it's a DataFrame) to Delta format and save it to MinIO\n",
    "# Create a Delta table from the DataFrame and overwrite if it exists\n",
    "combined_df.write.format(\"delta\").mode(\"overwrite\").save(f\"s3a://{minio_bucket_new}/combined_df\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
