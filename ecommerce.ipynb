{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORT LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from minio import Minio\n",
    "from minio.error import S3Error\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD BUCKET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blah\n",
      "climate-data\n",
      "ecommerce\n",
      "pythonminio\n",
      "File: 2019-Dec.csv\n",
      "File: 2019-Nov.csv\n",
      "File: 2020-Apr.csv\n",
      "File: 2020-Feb.csv\n",
      "File: 2020-Mar.csv\n"
     ]
    }
   ],
   "source": [
    "# Initialize MinIO client\n",
    "client = Minio('localhost:9000',\n",
    "                     access_key='minioadmin',\n",
    "                     secret_key='minioadmin',\n",
    "                     secure=False)  # Change to True if using HTTPS\n",
    "\n",
    "bucket_name = \"ecommerce\"\n",
    "\n",
    "# Example: List buckets\n",
    "try:\n",
    "    buckets = client.list_buckets()\n",
    "    for bucket in buckets:\n",
    "        print(bucket.name)\n",
    "except S3Error as e:\n",
    "    print(\"Error:\", e)\n",
    "\n",
    "try:\n",
    "    objects = client.list_objects(bucket_name)\n",
    "    for obj in objects:\n",
    "        if obj.object_name.endswith('/'):\n",
    "            print(\"Directory:\", obj.object_name)\n",
    "        else:\n",
    "            print(\"File:\", obj.object_name)\n",
    "except S3Error as e:\n",
    "    print(\"Error in listing objects:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 2] The system cannot find the file specified",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 23\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m     13\u001b[0m builder \u001b[38;5;241m=\u001b[39m SparkSession\u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39mappName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDelta With MinIO\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.hadoop.fs.s3a.endpoint\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp://minio:9000\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.hadoop.fs.s3a.access.key\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mminioadmin\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.sql.extensions\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mio.delta.sql.DeltaSparkSessionExtension\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.sql.catalog.spark_catalog\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morg.apache.spark.sql.delta.catalog.DeltaCatalog\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 23\u001b[0m spark \u001b[38;5;241m=\u001b[39m \u001b[43mconfigure_spark_with_delta_pip\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuilder\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Initialize Spark session\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;124;03m\"\"\" spark = SparkSession.builder \\\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;124;03m    .appName(\"Temperature Analysis with Delta\") \\\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;124;03m    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\") \\\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;124;03m    .getOrCreate()\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\vae.tiolamon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\sql\\session.py:497\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    495\u001b[0m     sparkConf\u001b[38;5;241m.\u001b[39mset(key, value)\n\u001b[0;32m    496\u001b[0m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[1;32m--> 497\u001b[0m sc \u001b[38;5;241m=\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[0;32m    499\u001b[0m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[0;32m    500\u001b[0m session \u001b[38;5;241m=\u001b[39m SparkSession(sc, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n",
      "File \u001b[1;32mc:\\Users\\vae.tiolamon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\context.py:515\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[1;34m(cls, conf)\u001b[0m\n\u001b[0;32m    513\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    514\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 515\u001b[0m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    517\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n",
      "File \u001b[1;32mc:\\Users\\vae.tiolamon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\context.py:201\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway\u001b[38;5;241m.\u001b[39mgateway_parameters\u001b[38;5;241m.\u001b[39mauth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    197\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    198\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not allowed as it is a security risk.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    199\u001b[0m     )\n\u001b[1;32m--> 201\u001b[0m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateway\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgateway\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_init(\n\u001b[0;32m    204\u001b[0m         master,\n\u001b[0;32m    205\u001b[0m         appName,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    215\u001b[0m         memory_profiler_cls,\n\u001b[0;32m    216\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\vae.tiolamon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\context.py:436\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    434\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    435\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_gateway:\n\u001b[1;32m--> 436\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_gateway \u001b[38;5;241m=\u001b[39m gateway \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mlaunch_gateway\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    437\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_gateway\u001b[38;5;241m.\u001b[39mjvm\n\u001b[0;32m    439\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m instance:\n",
      "File \u001b[1;32mc:\\Users\\vae.tiolamon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\java_gateway.py:100\u001b[0m, in \u001b[0;36mlaunch_gateway\u001b[1;34m(conf, popen_kwargs)\u001b[0m\n\u001b[0;32m     97\u001b[0m     proc \u001b[38;5;241m=\u001b[39m Popen(command, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpopen_kwargs)\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     99\u001b[0m     \u001b[38;5;66;03m# preexec_fn not supported on Windows\u001b[39;00m\n\u001b[1;32m--> 100\u001b[0m     proc \u001b[38;5;241m=\u001b[39m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpopen_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;66;03m# Wait for the file to appear, or for the process to exit, whichever happens first.\u001b[39;00m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m proc\u001b[38;5;241m.\u001b[39mpoll() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(conn_info_file):\n",
      "File \u001b[1;32mc:\\Users\\vae.tiolamon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\subprocess.py:1026\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize, process_group)\u001b[0m\n\u001b[0;32m   1022\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_mode:\n\u001b[0;32m   1023\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mTextIOWrapper(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr,\n\u001b[0;32m   1024\u001b[0m                     encoding\u001b[38;5;241m=\u001b[39mencoding, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m-> 1026\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreexec_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1027\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mpass_fds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1028\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshell\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1029\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mp2cread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp2cwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1030\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mc2pread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2pwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1031\u001b[0m \u001b[43m                        \u001b[49m\u001b[43merrread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1032\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mrestore_signals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1033\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mgid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mumask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1034\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstart_new_session\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocess_group\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1035\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m   1036\u001b[0m     \u001b[38;5;66;03m# Cleanup if the child failed starting.\u001b[39;00m\n\u001b[0;32m   1037\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstdin, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstdout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr)):\n",
      "File \u001b[1;32mc:\\Users\\vae.tiolamon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\subprocess.py:1538\u001b[0m, in \u001b[0;36mPopen._execute_child\u001b[1;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_gid, unused_gids, unused_uid, unused_umask, unused_start_new_session, unused_process_group)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# Start the process\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1538\u001b[0m     hp, ht, pid, tid \u001b[38;5;241m=\u001b[39m \u001b[43m_winapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCreateProcess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1539\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;66;43;03m# no special security\u001b[39;49;00m\n\u001b[0;32m   1540\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1541\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1542\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[43m                             \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1544\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1545\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1546\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   1547\u001b[0m     \u001b[38;5;66;03m# Child is launched. Close the parent's copy of those pipe\u001b[39;00m\n\u001b[0;32m   1548\u001b[0m     \u001b[38;5;66;03m# handles that only the child should have open.  You need\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;66;03m# pipe will not close when the child process exits and the\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m     \u001b[38;5;66;03m# ReadFile will hang.\u001b[39;00m\n\u001b[0;32m   1553\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_pipe_fds(p2cread, p2cwrite,\n\u001b[0;32m   1554\u001b[0m                          c2pread, c2pwrite,\n\u001b[0;32m   1555\u001b[0m                          errread, errwrite)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] The system cannot find the file specified"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "#import pyspark.errors\n",
    "from pyspark.sql.functions import col, max\n",
    "from delta import *\n",
    "\n",
    "# Stop the existing SparkContext if it exists\n",
    "try:\n",
    "    sc.stop()\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "\n",
    "builder = SparkSession.builder.appName(\"Delta With MinIO\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"minioadmin\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"minioadmin\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", True) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "\n",
    "# Initialize Spark session\n",
    "\"\"\" spark = SparkSession.builder \\\n",
    "    .appName(\"Temperature Analysis with Delta\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"minioadmin\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"minioadmin\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", True) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .getOrCreate()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS ecommerce\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# READ DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# einen test-DF erstellen, damit wir Delta ausprobieren kÃ¶nnen\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# Create a list of numbers from 1 to 10\n",
    "numbers = list(range(1, 11))\n",
    "\n",
    "# Convert the list to a DataFrame\n",
    "df = spark.createDataFrame([Row(number=n) for n in numbers])\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV files from MinIO bucket\n",
    "df_feb = spark.read.csv(f\"s3a://{bucket_name}/2020-Feb.csv\", header=True, inferSchema=True)\n",
    "df_mar = spark.read.csv(f\"s3a://{bucket_name}/2020-Mar.csv\", header=True, inferSchema=True)\n",
    "df_apr = spark.read.csv(f\"s3a://{bucket_name}/2020-Apr.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# December 2019 data\n",
    "try:\n",
    "    response_dec = client.get_object(bucket_name, \"2019-Dec.csv\")\n",
    "    dec_data = pd.read_csv(response_dec)   # this data will be used in step 8\n",
    "    print(dec_data.head())  # Display first few rows\n",
    "except S3Error as e:\n",
    "    print(\"Error in reading object:\", e)\n",
    "finally:\n",
    "    if 'response_dec' in locals():\n",
    "        response_dec.close()\n",
    "        response_dec.release_conn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# March 2020 data\n",
    "try:\n",
    "    response_mar = client.get_object(bucket_name, \"2020-Mar.csv\")\n",
    "    data_mar = pd.read_csv(response_mar)   # this data will be used in step 8\n",
    "    print(data_mar.head())  # Display first few rows\n",
    "except S3Error as e:\n",
    "    print(\"Error in reading object:\", e)\n",
    "finally:\n",
    "    if 'response_mar' in locals():\n",
    "        response_mar.close()\n",
    "        response_mar.release_conn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# April 2020 data\n",
    "try:\n",
    "    response_apr = client.get_object(bucket_name, \"2020-Apr.csv\")\n",
    "    data_apr = pd.read_csv(response_apr)   # this data will be used in step 8\n",
    "    print(data_apr.head())  # Display first few rows\n",
    "except S3Error as e:\n",
    "    print(\"Error in reading object:\", e)\n",
    "finally:\n",
    "    if 'response_apr' in locals():\n",
    "        response_apr.close()\n",
    "        response_apr.release_conn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# February 2020 data\n",
    "try:\n",
    "    response_feb = client.get_object(bucket_name, \"2020-Feb.csv\")\n",
    "    data_feb = pd.read_csv(response_feb)   # this data will be used in step 8\n",
    "    print(data_feb.head())  # Display first few rows\n",
    "except S3Error as e:\n",
    "    print(\"Error in reading object:\", e)\n",
    "finally:\n",
    "    if 'response_feb' in locals():\n",
    "        response_feb.close()\n",
    "        response_feb.release_conn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total number of rows in February 2020 data:\", len(data_feb))\n",
    "print(\"Total number of rows in March 2020 data:\", len(data_mar))\n",
    "print(\"Total number of rows in April 2020 data:\", len(data_apr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXPLORE THE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data_mar)\n",
    "\n",
    "# Filter numeric columns (integers or floats)\n",
    "numeric_columns = df.select_dtypes(include=['int', 'float']).columns\n",
    "\n",
    "# Create boxplot for each numeric column\n",
    "plt.figure(figsize=(10, 6))\n",
    "for column in numeric_columns:\n",
    "    plt.boxplot(df[column], labels=[column])\n",
    "    plt.title('Boxplot of Numeric Columns')\n",
    "    plt.ylabel('Values')\n",
    "    plt.xlabel('Columns')\n",
    "    plt.xticks(ticks=[1], labels=[column])  # Set column name as x-axis label\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data_apr)\n",
    "\n",
    "# Filter numeric columns (integers or floats)\n",
    "numeric_columns = df.select_dtypes(include=['int', 'float']).columns\n",
    "\n",
    "# Create boxplot for each numeric column\n",
    "plt.figure(figsize=(10, 6))\n",
    "for column in numeric_columns:\n",
    "    plt.boxplot(df[column], labels=[column])\n",
    "    plt.title('Boxplot of Numeric Columns')\n",
    "    plt.ylabel('Values')\n",
    "    plt.xlabel('Columns')\n",
    "    plt.xticks(ticks=[1], labels=[column])  # Set column name as x-axis label\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data_feb)\n",
    "\n",
    "# Filter numeric columns (integers or floats)\n",
    "numeric_columns = df.select_dtypes(include=['int', 'float']).columns\n",
    "\n",
    "# Create boxplot for each numeric column\n",
    "plt.figure(figsize=(10, 6))\n",
    "for column in numeric_columns:\n",
    "    plt.boxplot(df[column], labels=[column])\n",
    "    plt.title('Boxplot of Numeric Columns')\n",
    "    plt.ylabel('Values')\n",
    "    plt.xlabel('Columns')\n",
    "    plt.xticks(ticks=[1], labels=[column])  # Set column name as x-axis label\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# there is an outlier in the product_id column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_feb.info())\n",
    "print(data_mar.info())\n",
    "print(data_apr.info())\n",
    "\n",
    "print(data_feb.isnull().sum())\n",
    "print(data_mar.isnull().sum())\n",
    "print(data_apr.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Feb Data has {data_feb[\"category_code\"].value_counts().sum()} rows, in which {data_feb[\"category_code\"].isnull().sum()} of the data is null. Which means the non null data is {data_feb[\"category_code\"].value_counts().sum() - data_feb[\"category_code\"].isnull().sum()} rows or {((data_feb[\"category_code\"].value_counts().sum() - data_feb[\"category_code\"].isnull().sum())/data_feb[\"category_code\"].value_counts().sum())*100}% of the data\")\n",
    "print(f\"Mar Data has {data_mar[\"category_code\"].value_counts().sum()} rows, in which {data_mar[\"category_code\"].isnull().sum()} of the data is null. Which means the non null data is {data_mar[\"category_code\"].value_counts().sum() - data_mar[\"category_code\"].isnull().sum()} rows or {((data_mar[\"category_code\"].value_counts().sum() - data_mar[\"category_code\"].isnull().sum())/data_mar[\"category_code\"].value_counts().sum())*100}% of the data\")\n",
    "print(f\"Apr Data has {data_apr[\"category_code\"].value_counts().sum()} rows, in which {data_apr[\"category_code\"].isnull().sum()} of the data is null. Which means the non null data is {data_apr[\"category_code\"].value_counts().sum() - data_apr[\"category_code\"].isnull().sum()} rows or {((data_apr[\"category_code\"].value_counts().sum() - data_apr[\"category_code\"].isnull().sum())/data_apr[\"category_code\"].value_counts().sum())*100}% of the data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_feb[\"event_time\"] = pd.to_datetime(data_feb[\"event_time\"])\n",
    "data_mar[\"event_time\"] = pd.to_datetime(data_mar[\"event_time\"])\n",
    "data_apr[\"event_time\"] = pd.to_datetime(data_apr[\"event_time\"])\n",
    "\n",
    "data_feb[\"event_type\"] = data_feb[\"event_type\"].astype('category')\t\n",
    "data_mar[\"event_type\"] = data_mar[\"event_type\"].astype('category')\n",
    "data_apr[\"event_type\"] = data_apr[\"event_type\"].astype('category')\n",
    "\n",
    "data_feb[\"category_code\"] = data_feb[\"category_code\"].astype('category')\n",
    "data_mar[\"category_code\"] = data_mar[\"category_code\"].astype('category')\n",
    "data_apr[\"category_code\"] = data_apr[\"category_code\"].astype('category')\n",
    "\n",
    "\n",
    "\n",
    "print(data_feb.info())\n",
    "print(data_mar.info())\n",
    "print(data_apr.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_feb.describe())\n",
    "print(data_mar.describe())\n",
    "print(data_apr.describe())\n",
    "\n",
    "# Ratio of event types\n",
    "print(data_feb[\"event_type\"].value_counts(normalize=True))\t\n",
    "print(data_mar[\"event_type\"].value_counts(normalize=True))\n",
    "print(data_apr[\"event_type\"].value_counts(normalize=True))\n",
    "\n",
    "# Ratio of category codes\n",
    "print(data_feb[\"category_code\"].value_counts(normalize=True))\n",
    "print(data_mar[\"category_code\"].value_counts(normalize=True))\n",
    "print(data_apr[\"category_code\"].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(data_feb[\"event_type\"] == \"purchase\").sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_feb.groupby(\"event_type\").describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data_mar)\n",
    "\n",
    "# Calculate time intervals\n",
    "df['time_diff'] = df['event_time'].diff().fillna(pd.Timedelta(seconds=0))\n",
    "\n",
    "# Set threshold for outlier detection (e.g., 2 standard deviations from mean)\n",
    "threshold = df['time_diff'].mean() + 2 * df['time_diff'].std()\n",
    "\n",
    "# Find potential outliers (timestamps with intervals beyond threshold)\n",
    "outliers = df[df['time_diff'] > threshold]\n",
    "\n",
    "# Display potential outliers\n",
    "print(\"Potential Outliers:\")\n",
    "print(outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame (replace this with your dataset)\n",
    "data = {\n",
    "    'timestamp': [\n",
    "        '2023-01-01 08:00:00', '2023-01-01 08:30:00', '2023-01-01 09:00:00',\n",
    "        '2023-01-01 09:30:00', '2023-01-01 10:00:00', '2023-01-01 10:30:00'\n",
    "    ]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])  # Convert to datetime\n",
    "\n",
    "# Calculate time intervals\n",
    "df['time_diff'] = df['timestamp'].diff().fillna(pd.Timedelta(seconds=0))\n",
    "\n",
    "# Set threshold for outlier detection (e.g., 2 standard deviations from mean)\n",
    "threshold = df['time_diff'].mean() + 2 * df['time_diff'].std()\n",
    "\n",
    "# Find potential outliers (timestamps with intervals beyond threshold)\n",
    "outliers = df[df['time_diff'] > threshold]\n",
    "\n",
    "# Display potential outliers\n",
    "print(\"Potential Outliers:\")\n",
    "print(outliers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UPLOAD CLEANED DATA TO ANOTHER BUCKET TO BE USED FOR THE NEXT STEP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Assuming your cleaned data is stored in a dictionary\n",
    "cleaned_data = {\n",
    "    \"col1\": [1, 2, 3],\n",
    "    \"col2\": [\"A\", \"B\", \"C\"]\n",
    "}\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"Example\").getOrCreate()\n",
    "\n",
    "# Convert dictionary to DataFrame\n",
    "cleaned_data_df = spark.createDataFrame([(k, v) for k, v in cleaned_data.items()], [\"col_name\", \"col_data\"])\n",
    "\n",
    "# Save DataFrame to MinIO bucket as CSV\n",
    "bucket_name = \"your_bucket_name\"  # Replace with your bucket name\n",
    "minio_endpoint = \"minio_endpoint\"  # Replace with your MinIO endpoint\n",
    "\n",
    "cleaned_data_df.write.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(f\"s3a://{minio_endpoint}/{bucket_name}/cleaned_data.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
